{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary files here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "#import os\n",
    "#import json\n",
    "import string\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(filename):\n",
    "    print(filename+\": \\n\")\n",
    "    with io.open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "        corpus = f.read()\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    dirt = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.-_/\\~`*%$#@,:;1234567890<>(){}[]‘’''১২৩৪৫৬৭৮৯০\"\n",
    "    data = [char for char in data if char not in dirt]\n",
    "    data = ''.join(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_daari(corpus):\n",
    "    corpus = corpus.replace('\\n',' ')\n",
    "    corpus = re.split('।+|\\?+|\\!+',corpus)\n",
    "    #corpus = [s for s in re.split('।|\\?|\\!',corpus) if s]\n",
    "    corpus = [sentence.strip() for sentence in corpus]\n",
    "    data = [sentence.split() for sentence in corpus]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data(data):\n",
    "    print(type(data))\n",
    "    print(len(data))\n",
    "    print(data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def array_handler(array):\n",
    "#     dirt = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.-,?:1234567890(){}[]‘’''১২৩৪৫৬৭৮৯০‘\"\n",
    "#    data = [char for char in data if char not in dirt]\n",
    "#     return \"\".join(data)\n",
    "\n",
    "# def clean_data_multiprocess(data):\n",
    "#     len_data = len(data)\n",
    "#     split_range = len_data//20\n",
    "#     result = \"\"\n",
    "#     for indx in range(20):\n",
    "        \n",
    "#         if ((indx+1)*split_range)>=len_data:\n",
    "#             res = array_handle(data[(indx*split_range): ])\n",
    "#             result += res\n",
    "#         else:\n",
    "#             res = array_handler(data[(indx*split_range):((indx+1)*split_range)])\n",
    "#             result += res\n",
    "            \n",
    "#     # dirt = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.-,?:1234567890(){}[]‘’''১২৩৪৫৬৭৮৯০‘\"\n",
    "#     # data = [char for char in data if char not in dirt]data = ''.join(data)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_with_newline(corpus):\n",
    "#    corpus = corpus.split(\"\\n\")\n",
    "#    corpus = [sentence.strip() for sentence in corpus]\n",
    "#    data = [sentence.split() for sentence in corpus]\n",
    "#    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_stopword(data):\n",
    "#     #stop word list\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stemming(data):\n",
    "#     #rafi stemmer\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessing(filename, sentence_separation, preprocess = False):\n",
    "#     corpus = load_corpus(filename)\n",
    "#     print_data(corpus)\n",
    "#     if not preprocess:\n",
    "#         corpus = clean_data(corpus)\n",
    "#     else:\n",
    "#         corpus = clean_data_multiprocess(corpus)\n",
    "#     print_data(corpus)\n",
    "#     if sentence_separation == 'daari_seperated':\n",
    "#         corpus = tokenize_with_daari(corpus)\n",
    "#     elif sentence_separation=='newline_seperated':\n",
    "#         corpus = tokenize_with_newline(corpus)\n",
    "#     print_data(corpus)\n",
    "#     total_data.extend(corpus)\n",
    "#     print(len(total_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing('newfile.txt','daari_seperated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocessing('prothom_alo.txt','newline_seperated')\n",
    "# preprocessing('book1.txt','daari_seperated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pkl_file(filename):\n",
    "    pkl_file = open(filename, 'rb')\n",
    "    file_data = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    #print(\"data\")\n",
    "    #print_data(file_data)\n",
    "    \n",
    "    corpus = ''.join(file_data)\n",
    "    #print(\"data to string\")\n",
    "    #print(type(corpus))\n",
    "    \n",
    "    corpus = clean_data(corpus)\n",
    "    #print(\"after clean\")\n",
    "    #print(type(corpus))\n",
    "    #print(corpus[1:1000])\n",
    "    \n",
    "    corpus = tokenize_with_daari(corpus)\n",
    "    #print(\"after tokenize\")\n",
    "    print_data(corpus)\n",
    "    \n",
    "    total_data.extend(corpus)\n",
    "    print(len(total_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global total_data\n",
    "### data after preprocessing is stored here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and view pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pkl_file(\"Corpus\\ittefaq.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pkl_file(\"Corpus\\kalerkantho.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pkl_file(\"Corpus\\sachalayatan.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pkl_file(\"Corpus\\_bnwiki.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pkl_file(\"Corpus\\_banglapedia.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pkl_file(\"Corpus\\_bangla_books.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_in_pkl(data_2Dlist, filename):\n",
    "    outfile = open(filename,'wb')\n",
    "    pickle.dump(data_2Dlist,outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after writing a file, comment out the code for writing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the pkl file is created after cleaning with some symbols(as suravi apu did) and splitting sentence with daari\n",
    "# ittefaq, kalerkantho, sachalayaton, bangla_books, banglapedia, bnwiki\n",
    "# print_data(total_data)\n",
    "### filename = 'cleaned\\outfile_iksbbb'\n",
    "### write_in_pkl(total_data,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Corpus\\cleaned_split_with_three_symbol\\i_k_s_b_b_b.pkl\"\n",
    "write_in_pkl(total_data,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "del total_data[:]\n",
    "del total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_large_pkl_file(filename,split_size):\n",
    "    pkl_file = open(filename, 'rb')\n",
    "    file_data = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print(len(file_data))\n",
    "    #print_data(file_data)\n",
    "    \n",
    "    split = int(len(file_data)/split_size)\n",
    "    #print(type(split))\n",
    "    iterator = 0\n",
    "    corpus = ''\n",
    "    for i in range(split_size):\n",
    "        if ((iterator+split)>len(file_data)):\n",
    "            data = file_data[iterator:]\n",
    "        else:\n",
    "            data = file_data[iterator:(iterator+split)]\n",
    "        corpus = clean_data(''.join(data))\n",
    "        #print(corpus[1:1000])\n",
    "        corpus = tokenize_with_daari(corpus)\n",
    "        #print(corpus[1:10])\n",
    "        total_data.extend(corpus)\n",
    "        print(len(total_data))\n",
    "        iterator += split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142220\n",
      "1303068\n",
      "2604491\n",
      "3917975\n"
     ]
    }
   ],
   "source": [
    "total_data = []\n",
    "preprocessing_large_pkl_file(\"Corpus\\prothomalo.pkl\",3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'Corpus\\cleaned\\outfile_pa'\n",
    "# outfile = open(filename,'wb')\n",
    "# pickle.dump(total_data,outfile)\n",
    "# outfile.close()\n",
    "# print_data(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Corpus\\cleaned_split_with_three_symbol\\prothomalo.pkl\"\n",
    "write_in_pkl(total_data,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del total_data[:]\n",
    "del total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_large_pkl_file_somewherein(filename,split_size):\n",
    "    pkl_file = open(filename, 'rb')\n",
    "    file_data = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print(len(file_data))\n",
    "    print_data(file_data)\n",
    "    \n",
    "    split = int(len(file_data)/split_size)\n",
    "    print(\"split size \" + str(split))\n",
    "    iterator = 0\n",
    "    corpus = ''\n",
    "    for i in range(split_size):\n",
    "        if ((iterator+split)>len(file_data)):\n",
    "            data = file_data[iterator:]\n",
    "        else:\n",
    "            data = file_data[iterator:(iterator+split)]\n",
    "        corpus = clean_data(''.join(data))\n",
    "        #print(corpus[1:1000])\n",
    "        corpus = tokenize_with_daari(corpus)\n",
    "        #print(corpus[1:10])\n",
    "        write_in_pkl(corpus, (\"Corpus\\cleaned_split_with_three_symbol\\somewherein\\somewherein\"+str(i)+\".pkl\"))\n",
    "        #total_data.extend(corpus)\n",
    "        #print(len(total_data))\n",
    "        iterator += split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = []\n",
    "preprocessing_large_pkl_file_somewherein(\"Corpus\\somewherein-003.pkl\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"Corpus\\cleaned_split_with_three_symbol\\somewherein.pkl\"\n",
    "#write_in_pkl(total_data,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del total_data[:]\n",
    "del total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'Corpus\\cleaned\\outfile_somewherein'\n",
    "# outfile = open(filename,'wb')\n",
    "# pickle.dump(total_data,outfile)\n",
    "# outfile.close()\n",
    "# print_data(total_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing DONE!!!\n",
    "\n",
    "### Now start Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-dimensional list\n",
    "def open_preprocessed_pkl_file(filename):\n",
    "    pkl_file = open(filename, 'rb')\n",
    "    file_data = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    print(len(file_data))\n",
    "    print_data(file_data)\n",
    "    total_data.extend(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open_preprocessed_pkl_file(\"Corpus\\cleaned\\outfile_pa\")\n",
    "open_preprocessed_pkl_file(\"Corpus\\cleaned_split_with_three_symbol\\prothomalo.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open_preprocessed_pkl_file(\"Corpus\\cleaned\\outfile_iksbbb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_preprocessed_pkl_file(\"Corpus\\cleaned_split_with_three_symbol\\i_k_s_b_b_b.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    open_preprocessed_pkl_file((\"Corpus\\cleaned_split_with_three_symbol\\somewherein\\somewherein\"+str(i)+\".pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(algorithm, alg_type):\n",
    "    if algorithm == 'word2vec':\n",
    "        if alg_type == 'cbow':\n",
    "            model = Word2Vec(total_data, size=200, window=5, min_count=5, workers=4, sg=0)\n",
    "            return model\n",
    "        elif alg_type == 'skipgram': \n",
    "            model = Word2Vec(total_data, size=200, window=5, min_count=5, workers=4, sg=1)\n",
    "            return model\n",
    "    elif algorithm == 'fastText':\n",
    "        if alg_type == 'cbow':\n",
    "            model = FastText(total_data, size=100, window=5, min_count=5, workers=4,sg=0)\n",
    "            return model\n",
    "        elif alg_type == 'skipgram':\n",
    "            model = FastText(total_data, size=100, window=5, min_count=5, workers=4,sg=1)\n",
    "            return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the embeddings and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = embedding('word2vec', 'cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1.wv.save_word2vec_format(\"model\\\\without preprocessing\\model1.txt\", binary=False)\n",
    "#model1.wv.save_word2vec_format(\"model\\\\after cleaned\\model1.bin\")\n",
    "#model1.wv.save_word2vec_format(\"model\\\\after cleaned_split_with_three_symbol\\model1.bin\")\n",
    "model1.wv.save_word2vec_format(\"model\\\\after cleaned_split_with_three_symbol_v2\\model1.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('বাংলাদেশের', 0.6704822778701782), ('পাকিস্তান', 0.6285028457641602), ('শ্রীলঙ্কা', 0.5476563572883606), ('ভারত', 0.5461157560348511), ('বাংলাদেশর', 0.5328458547592163)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val1 = model1.wv.most_similar('বাংলাদেশ', topn=5)\n",
    "print(val1)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = embedding('word2vec', 'skipgram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv.save_word2vec_format(\"model\\\\after cleaned_split_with_three_symbol_v2\\model2.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv.save_word2vec_format(\"model2.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = embedding('fastText', 'cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.wv.save_word2vec_format(\"model\\\\after cleaned_split_with_three_symbol_v2\\model3.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = embedding('fastText', 'skipgram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.wv.save_word2vec_format(\"model\\\\after cleaned_split_with_three_symbol_v2\\model4.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model5 = fasttext.load_model('cc.bn.300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8844967\n",
      "\n",
      "0.81329715\n",
      "\n",
      "0.91883695\n",
      "\n",
      "0.8536207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val1 = model1.wv.similarity('বাবা','মা')\n",
    "print(val1)\n",
    "print()\n",
    "val2 = model2.wv.similarity('বাবা','মা')\n",
    "print(val2)\n",
    "print()\n",
    "val3 = model3.wv.similarity('বাবা','মা')\n",
    "print(val3)\n",
    "print()\n",
    "val4 = model4.wv.similarity('বাবা','মা')\n",
    "print(val4)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15436402\n",
      "\n",
      "0.26775798\n",
      "\n",
      "0.17564707\n",
      "\n",
      "0.47193083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val1 = model1.wv.similarity('বাবা','বই')\n",
    "print(val1)\n",
    "print()\n",
    "val2 = model2.wv.similarity('বাবা','বই')\n",
    "print(val2)\n",
    "print()\n",
    "val3 = model3.wv.similarity('বাবা','বই')\n",
    "print(val3)\n",
    "print()\n",
    "val4 = model4.wv.similarity('বাবা','বই')\n",
    "print(val4)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09268651\n",
      "\n",
      "0.3654198\n",
      "\n",
      "0.3460052\n",
      "\n",
      "0.55172795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val1 = model1.wv.similarity('নারী','ছেলে')\n",
    "print(val1)\n",
    "print()\n",
    "val2 = model2.wv.similarity('নারী','ছেলে')\n",
    "print(val2)\n",
    "print()\n",
    "val3 = model3.wv.similarity('নারী','ছেলে')\n",
    "print(val3)\n",
    "print()\n",
    "val4 = model4.wv.similarity('নারী','ছেলে')\n",
    "print(val4)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68630016\n",
      "\n",
      "0.68622607\n",
      "\n",
      "0.783335\n",
      "\n",
      "0.7825465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val1 = model1.wv.similarity('কবি','লেখক')\n",
    "print(val1)\n",
    "print()\n",
    "val2 = model2.wv.similarity('কবি','লেখক')\n",
    "print(val2)\n",
    "print()\n",
    "val3 = model3.wv.similarity('কবি','লেখক')\n",
    "print(val3)\n",
    "print()\n",
    "val4 = model4.wv.similarity('কবি','লেখক')\n",
    "print(val4)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777735\n",
      "\n",
      "0.7588824\n",
      "\n",
      "0.7881891\n",
      "\n",
      "0.809259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val1 = model1.wv.similarity('ফুটবল','ক্রিকেট')\n",
    "print(val1)\n",
    "print()\n",
    "val2 = model2.wv.similarity('ফুটবল','ক্রিকেট')\n",
    "print(val2)\n",
    "print()\n",
    "val3 = model3.wv.similarity('ফুটবল','ক্রিকেট')\n",
    "print(val3)\n",
    "print()\n",
    "val4 = model4.wv.similarity('ফুটবল','ক্রিকেট')\n",
    "print(val4)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('বইও', 0.8290922045707703), ('বইগুলো', 0.7868694067001343), ('বইটি', 0.7650830745697021), ('বইটা', 0.7423145771026611), ('বইই', 0.7379765510559082)]\n",
      "\n",
      "[('বইও', 0.775334894657135), ('বইগুলো', 0.7651818990707397), ('পুস্তক', 0.7174132466316223), ('বইপত্র', 0.7080742120742798), ('বইটা', 0.7031193375587463)]\n",
      "\n",
      "[('বইবই', 0.9280502796173096), ('বই…', 0.920569658279419), ('বইÑ', 0.9164352416992188), ('বই—', 0.9158008098602295), ('বই৷', 0.9100830554962158)]\n",
      "\n",
      "[('বইগুলো', 0.8601911067962646), ('বইগুল', 0.8463097810745239), ('লেখাপত্র', 0.8436557650566101), ('বইপত্র', 0.8400493860244751), ('বইটি', 0.8321079015731812)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val1 = model1.wv.most_similar('বই', topn=5)\n",
    "print(val1)\n",
    "print()\n",
    "val2 = model2.wv.most_similar('বই', topn=5)\n",
    "print(val2)\n",
    "print()\n",
    "val3 = model3.wv.most_similar('বই', topn=5)\n",
    "print(val3)\n",
    "print()\n",
    "val4 = model4.wv.most_similar('বই', topn=5)\n",
    "print(val4)\n",
    "print()\n",
    "#val5 = model5.get_nearest_neighbors('বই')\n",
    "#print(val5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('পেন্সিল', 0.6875611543655396), ('পেনসিল', 0.6740241646766663), ('কলমও', 0.6707340478897095), ('কলমের', 0.640700101852417), ('বলপয়েন্ট', 0.6326653957366943)]\n",
      "\n",
      "[('কলমও', 0.7381693124771118), ('পেন্সিল', 0.6929911971092224), ('কলমের', 0.6923819780349731), ('খাগের', 0.6782441139221191), ('পেনসিল', 0.6737237572669983)]\n",
      "\n",
      "[('কলম”', 0.8817577362060547), ('কলম\"', 0.8604620099067688), ('কলমচি', 0.8320593237876892), ('লওহকলম', 0.8273812532424927), ('কলমকাগজ', 0.8208644986152649)]\n",
      "\n",
      "[('কলমও', 0.8217117786407471), ('কলমখাতা', 0.8197134733200073), ('কলমকাগজ', 0.8142791986465454), ('কলমপেন্সিল', 0.7977180480957031), ('খাতাকলমচকপেনসিল', 0.7906641960144043)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val1 = model1.wv.most_similar('কলম', topn=5)\n",
    "print(val1)\n",
    "print()\n",
    "val2 = model2.wv.most_similar('কলম', topn=5)\n",
    "print(val2)\n",
    "print()\n",
    "val3 = model3.wv.most_similar('কলম', topn=5)\n",
    "print(val3)\n",
    "print()\n",
    "val4 = model4.wv.most_similar('কলম', topn=5)\n",
    "print(val4)\n",
    "print()\n",
    "#val5 = model5.get_nearest_neighbors('কলম')\n",
    "#print(val5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('নজরূল', 0.768830418586731), ('সিরাজুল', 0.7211235761642456), ('রফিকুল', 0.701414942741394), ('নজরম্নল', 0.7011711597442627), ('মযহারুল', 0.7001498937606812)]\n",
      "\n",
      "[('কাজী', 0.8067245483398438), ('রফিকুল', 0.7441709637641907), ('ইসলামগীতিকারঃ', 0.7411827445030212), ('শফিকুল', 0.7387590408325195), ('কবিকাজী', 0.7370297908782959)]\n",
      "\n",
      "[('”নজরুল', 0.976799488067627), ('“নজরুল', 0.9705806374549866), ('মনজরুল', 0.9690474271774292), ('\"নজরুল', 0.967396080493927), ('মোনজরুল', 0.9569004774093628)]\n",
      "\n",
      "[('মনজরুল', 0.9480078816413879), ('”নজরুল', 0.9310480356216431), ('মোনজরুল', 0.9277504682540894), ('রবিনজরুল', 0.9189256429672241), ('কাজীনজরুল', 0.9168095588684082)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val1 = model1.wv.most_similar('নজরুল', topn=5)\n",
    "print(val1)\n",
    "print()\n",
    "val2 = model2.wv.most_similar('নজরুল', topn=5)\n",
    "print(val2)\n",
    "print()\n",
    "val3 = model3.wv.most_similar('নজরুল', topn=5)\n",
    "print(val3)\n",
    "print()\n",
    "val4 = model4.wv.most_similar('নজরুল', topn=5)\n",
    "print(val4)\n",
    "print()\n",
    "#val5 = model5.get_nearest_neighbors('নজরুল')\n",
    "#print(val5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('খাইনা', 0.796798825263977), ('খাচ্ছি', 0.7853677272796631), ('খেতাম', 0.7816851139068604), ('খাবো', 0.7630881071090698), ('খাব', 0.7550476789474487)]\n",
      "\n",
      "[('খাবো', 0.7620536684989929), ('খাইনা', 0.7574131488800049), ('খাচ্ছি', 0.7486290335655212), ('খেতাম', 0.7473747134208679), ('খাব', 0.7437191009521484)]\n",
      "\n",
      "[('খাইখাই', 0.9135904312133789), ('খাই৷', 0.8900064826011658), ('খাই”', 0.8722144365310669), ('খাইআর', 0.8700817823410034), ('খাবোই', 0.8698590397834778)]\n",
      "\n",
      "[('খাইনা', 0.869753897190094), ('খাচ্ছি', 0.8521871566772461), ('খায়', 0.8434346914291382), ('খাব', 0.8349928855895996), ('খাও', 0.8166855573654175)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val1 = model1.wv.most_similar('খাই', topn=5)\n",
    "print(val1)\n",
    "print()\n",
    "val2 = model2.wv.most_similar('খাই', topn=5)\n",
    "print(val2)\n",
    "print()\n",
    "val3 = model3.wv.most_similar('খাই', topn=5)\n",
    "print(val3)\n",
    "print()\n",
    "val4 = model4.wv.most_similar('খাই', topn=5)\n",
    "print(val4)\n",
    "print()\n",
    "#val5 = model5.get_nearest_neighbors('খাই')\n",
    "#print(val5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ঠিকানাও', 0.7837664484977722), ('ঠিকানাটা', 0.7134160995483398), ('এড্রেস', 0.7119320631027222), ('নামঠিকানা', 0.6800862550735474), ('ঠিকানাই', 0.6607719659805298)]\n",
      "\n",
      "[('ঠিকানাও', 0.7962367534637451), ('ঠিকানাই', 0.7232276201248169), ('ঠিকানাটা', 0.6977918744087219), ('ঠিকানার', 0.6953620910644531), ('নামঠিকানা', 0.675594687461853)]\n",
      "\n",
      "[('\"ঠিকানা', 0.9420958757400513), ('ঠিকানাঃ“', 0.935348629951477), ('নাম–ঠিকানা', 0.928582489490509), ('ঠিকানাএ', 0.9270721077919006), ('নাঠিকানা', 0.9182928800582886)]\n",
      "\n",
      "[('ডাকঠিকানা', 0.9215134382247925), ('ঠিকানাও', 0.9150959253311157), ('ঠিকানাই', 0.9131706953048706), ('নাম–ঠিকানা', 0.911676824092865), ('ঠিকানা৷', 0.9024336338043213)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val1 = model1.wv.most_similar('ঠিকানা', topn=5)\n",
    "print(val1)\n",
    "print()\n",
    "val2 = model2.wv.most_similar('ঠিকানা', topn=5)\n",
    "print(val2)\n",
    "print()\n",
    "val3 = model3.wv.most_similar('ঠিকানা', topn=5)\n",
    "print(val3)\n",
    "print()\n",
    "val4 = model4.wv.most_similar('ঠিকানা', topn=5)\n",
    "print(val4)\n",
    "print()\n",
    "#val5 = model5.get_nearest_neighbors('ঠিকানা')\n",
    "#print(val5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('রাজশাহী', 0.8118138909339905), ('চট্টগ্রাম', 0.7809731364250183), ('কুমিল্লা', 0.760356605052948), ('খুলনা', 0.7367624044418335), ('সিলেট', 0.7261035442352295)]\n",
      "\n",
      "[('চট্টগ্রাম', 0.7976619005203247), ('রাজশাহী', 0.7375864386558533), ('খুলনা', 0.7145917415618896), ('বিজ্ঞপ্তিঢাকা', 0.7000488042831421), ('কুমিল্লা', 0.6962804198265076)]\n",
      "\n",
      "[('\"ঢাকা', 0.9555426239967346), ('েঢাকা', 0.9513428211212158), ('“ঢাকা', 0.9496367573738098), ('এঢাকা', 0.9483811855316162), ('|ঢাকা', 0.9475576877593994)]\n",
      "\n",
      "[('চট্টগ্রাম', 0.9039357900619507), ('রাজশাহী', 0.8715474605560303), ('বিশ্ববিদ্যালয়চট্টগ্রাম', 0.8526240587234497), ('খুলনা', 0.8518131971359253), ('ঢাকার', 0.8364359140396118)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val1 = model1.wv.most_similar('ঢাকা', topn=5)\n",
    "print(val1)\n",
    "print()\n",
    "val2 = model2.wv.most_similar('ঢাকা', topn=5)\n",
    "print(val2)\n",
    "print()\n",
    "val3 = model3.wv.most_similar('ঢাকা', topn=5)\n",
    "print(val3)\n",
    "print()\n",
    "val4 = model4.wv.most_similar('ঢাকা', topn=5)\n",
    "print(val4)\n",
    "print()\n",
    "#val5 = model5.get_nearest_neighbors('ঢাকা')\n",
    "#print(val5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embedding Models  and Make Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = KeyedVectors.load_word2vec_format(\"model1.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = KeyedVectors.load_word2vec_format(\"model2.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = KeyedVectors.load_word2vec_format(\"model3.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = KeyedVectors.load_word2vec_format(\"model4.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarity(model, string1, string2):\n",
    "    val1 = model.similarity(string1, string2)\n",
    "    print('%.2f' % val1)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_equation(model,positive,negative):\n",
    "    val = model.most_similar(positive, negative,topn=5)\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words(model, string):\n",
    "    val1 = model.most_similar(string, topn=3)\n",
    "    print(val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('রাণী', 0.6641949415206909)]\n",
      "[('রানি', 0.6961914300918579)]\n",
      "[('রাজারাণী', 0.8331283330917358)]\n",
      "[('রানি', 0.8929707407951355)]\n"
     ]
    }
   ],
   "source": [
    "word_equation(model1, positive=['রাজা', 'মেয়ে'], negative=['ছেলে'])\n",
    "word_equation(model2, positive=['রাজা', 'মেয়ে'], negative=['ছেলে'])\n",
    "word_equation(model3, positive=['রাজা', 'মেয়ে'], negative=['ছেলে'])\n",
    "word_equation(model4, positive=['রাজা', 'মেয়ে'], negative=['ছেলে'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('মা', 0.8470655679702759)]\n",
      "[('মা', 0.841377317905426)]\n",
      "[('মা', 0.8503274321556091)]\n",
      "[('মা', 0.8910645246505737)]\n"
     ]
    }
   ],
   "source": [
    "word_equation(model1, positive=['বাবা', 'মেয়ে'], negative=['ছেলে'])\n",
    "word_equation(model2, positive=['বাবা', 'মেয়ে'], negative=['ছেলে'])\n",
    "word_equation(model3, positive=['বাবা', 'মেয়ে'], negative=['ছেলে'])\n",
    "word_equation(model4, positive=['বাবা', 'মেয়ে'], negative=['ছেলে'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('লেখিকা', 0.724480390548706)]\n",
      "[('ঔপন্যাসিক', 0.618572473526001)]\n",
      "[('জাহাঙ্গীরনগর', 0.46925368905067444)]\n",
      "[('দেবেন্দ্রনাথ', 0.7047825455665588)]\n",
      "[('বঙ্গবন্ধুকন্যা', 0.6981199979782104)]\n"
     ]
    }
   ],
   "source": [
    "word_equation(model1, positive=['লেখক', 'নারী'], negative=[])\n",
    "word_equation(model2, positive=['লেখিকা','পুরুষ'], negative=['নারী'])\n",
    "word_equation(model1, positive=['ঢাকা'], negative=['রাজধানী'])\n",
    "word_equation(model1, positive=['রবীন্দ্রনাথ','ভ্রাতা'], negative=[])\n",
    "word_equation(model1, positive=['শেখ','হাসিনা'], negative=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('রাজশাহী', 0.8118138909339905), ('চট্টগ্রাম', 0.7809731364250183), ('কুমিল্লা', 0.760356605052948)]\n",
      "[('কাজী', 0.8067245483398438), ('রফিকুল', 0.7441709637641907), ('ইসলামগীতিকারঃ', 0.7411827445030212)]\n",
      "[('কলমও', 0.7381693124771118), ('পেন্সিল', 0.6929911971092224), ('কলমের', 0.6923819780349731)]\n",
      "[('বুধবার', 0.995028018951416), ('মঙ্গলবার', 0.9946527481079102), ('বৃহস্পতিবার', 0.9931206703186035)]\n"
     ]
    }
   ],
   "source": [
    "find_similar_words(model1,'ঢাকা')\n",
    "find_similar_words(model2,'নজরুল')\n",
    "find_similar_words(model2,'কলম')\n",
    "find_similar_words(model4, 'সোমবার')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50\n",
      "\n",
      "0.67\n",
      "\n",
      "0.74\n",
      "\n",
      "0.81\n",
      "\n",
      "0.27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_similarity(model1, 'বিশ্ববিদ্যালয়', 'স্কুল')\n",
    "find_similarity(model1, 'বিশ্ববিদ্যালয়', 'কলেজ')\n",
    "find_similarity(model1, 'কলেজ', 'স্কুল')\n",
    "find_similarity(model2, 'বাবা', 'মা')\n",
    "find_similarity(model2, 'বাবা', 'বই')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('বেড়াল', 0.8566687107086182), ('কুকুর', 0.7520319223403931), ('ইদুর', 0.731909453868866)]\n",
      "[('পোষা', 0.7915866374969482), ('কুকুরের', 0.787040650844574), ('কুকুরকে', 0.7502276301383972)]\n",
      "[('বিড়ালএ', 0.9498353600502014), ('বিড়ালি', 0.9380178451538086), ('বিড়াল\"', 0.9302704334259033)]\n",
      "[('কুকুরলেজ', 0.9136520624160767), ('কুকুর৷', 0.9062893986701965), ('কুকুরি', 0.8912427425384521)]\n"
     ]
    }
   ],
   "source": [
    "find_similar_words(model1, 'বিড়াল')\n",
    "find_similar_words(model2, 'কুকুর')\n",
    "find_similar_words(model3, 'বিড়াল')\n",
    "find_similar_words(model4, 'কুকুর')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
